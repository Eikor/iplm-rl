# iPLM-RL ðŸ§¬ âŸ³ ðŸ¤–
Offical code for paper [Protein Inverse Folding From Structure Feedback](https://arxiv.org/abs/2506.03028).

## About
This codebase is build mainly upon the [TRL](https://github.com/huggingface/trl) framework, long live the community! 

Before start, setup enviornment using `requirments.txt`.

To lower the cost from fresh start, the training data used in the paper can be download in [Google Drive](https://drive.google.com/file/d/1_TRdJRmsVOf2c-9n4BbLMv5HjsMNx5z1/view?usp=sharing).
This data contains over 1.8M protein sequence generated by InstructPLM and corresponding TM-Scores of predicted structures by ESMFold.

The ckpt used in our paper is 

The pre-generated dataset are organized as follows:
```python
[
    [PDBid, WTseq, [TM-Scores norm1], [TM-Scores norm2], [generated sequences]],
    [PDBid, WTseq, [TM-Scores norm1], [TM-Scores norm2], [generated sequences]],
    ...
]
```
> [!NOTE]
> TM-Scores norm1 and norm2 represents the TM-Scores normalized by predicted structures or ground-truth structures, in our paper, we report the TM-Scores norm2 as final results.


## Full process step by step

### Construct HF dataset
1. Preporcess structure embedding by [InstructPLM](https://github.com/Eikor/InstructPLM), store them in **structure_path**.

2. Modity **structure_path** and **model_path** in `generate.sh`, generate sequences using 
    ```bash
    bash generate.sh \
        [total] \ # number of generated sequences per structure
        [temperature] \ # temperature when generate
        [adapter_path] \ # lora adatper path, '-' for no pretrained lora adapter
        [save_prefix] # save folder name
    ```
    This command should generated fasta files organized as follows:
    ```
    save_prefix
    â”‚   pdb_res1.fasta
    â”‚   pdb_res2.fasta
    â”‚   pdb_res3.fasta
    â”‚   ...
    ```
    
3. Folding generated sequences using ESMFold.
    ```bash
    ./fold.sh \
        [fasta_path] \ # generated sequences path
        [save_path] # predicted pdb saving path
    ```
    This command should generated pdb files organized as follows:
    ```
    save_path
    â”‚
    â””â”€â”€â”€pdb1
    â”‚   â”‚   0.pdb
    â”‚   â”‚   1.pdb
    â”‚   â”‚   ...
    â”‚   
    â””â”€â”€â”€pdb2
        â”‚   0.pdb
        â”‚   1.pdb
        |   ...
        ...
    ```
4. Calculate TM-Scores using TMAlign.
    ```bash
    python utils/tmalign.py \
        [gt_pdb_path] \ # ground-truth pdb file path
        [pred_pdb_path] \ # predicted pdb file path
        [save_name] \ # save name
    ```
    This command should generated pkl files stores list of TM-Scores. 

5. Construct HF dataset.
    > [!NOTE]
    > If you have download the preprocessed training set, you can start from here, with a little modification of line 35 of `build_dataset.py`.

    ``` bash
    python utils/build_dataset.py \
        [subset] \ # numbers of each 
        [tmscore_path] \ # pkl file path of last step 
        [save_name] \ # save name of final HF dataset
    ```


### DPO Training
1. Download InstructPLM ckpt from [huggingface](https://huggingface.co/InstructPLM/Concated-Progen2-xlarge-CATH42-AFDB/tree/main), replace the tokenizer file with our newer version of [`tokenizer_iPLM.py`](utils/tokenization_iPLM.py)
2. modify the **structure_emb_path** and the **model_name_or_path** in [`dpo.sh`](dpo.sh).
3. Run experiments by 
    ```bash
    bash dpo.sh \
        [ALPHA] \ # weights to balance positive and negative samples
        [TRAIN STEPS] \ # total train steps
        [RUN NAME] \ # run name
        [PRETRAINED ADAPTER] \ # pretrained weights, used in multiround, "" for fresh start
        [HF DATASET] \ # HF data path
    ```
    
### Multi-round DPO
Run experiments by
```bash
bash multi-round.sh \
    [ROUND_NUM] \ # number of rounds
    [ALPHA] \ # weights to balance positive and negative samples
    [STEP] \ # train steps of each round
    [subset] \ # number of samples generated in each round
    [gt_pdb_path] \ # ground-truth pdb file path
```
